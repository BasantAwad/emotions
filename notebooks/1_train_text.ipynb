{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Emotion Training - Multi-Dataset\n",
    "\n",
    "Trains RoBERTa on multiple text emotion datasets with unified labels.\n",
    "\n",
    "**Datasets:**\n",
    "- simaanjali/emotion-analysis-based-on-text\n",
    "- nelgiriyewithana/emotions\n",
    "- GoEmotions (HuggingFace)\n",
    "\n",
    "**Run in Google Colab with GPU!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1: Setup Kaggle API\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "# Create kaggle.json with your API key\n",
    "kaggle_json = '''{\n",
    "    \"username\": \"basantawad\",\n",
    "    \"key\": \"73699caea5f0322acca5bc42516c5998\"\n",
    "}'''\n",
    "\n",
    "kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "kaggle_file = os.path.join(kaggle_dir, 'kaggle.json')\n",
    "with open(kaggle_file, 'w') as f:\n",
    "    f.write(kaggle_json)\n",
    "os.chmod(kaggle_file, 0o600)\n",
    "\n",
    "print('Kaggle API Connected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2: Install Libraries\n",
    "# ============================================================\n",
    "!pip install transformers datasets accelerate kaggle pandas scikit-learn numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: Download Datasets from Kaggle\n",
    "# ============================================================\n",
    "!mkdir -p ./datasets/text\n",
    "\n",
    "# Dataset 1: Emotion Analysis Based on Text\n",
    "!kaggle datasets download -d simaanjali/emotion-analysis-based-on-text -p ./datasets/text/simaanjali --unzip\n",
    "\n",
    "# Dataset 2: Emotions Dataset\n",
    "!kaggle datasets download -d nelgiriyewithana/emotions -p ./datasets/text/emotions --unzip\n",
    "\n",
    "print('\\nDatasets downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4: Label Translation Map (CRITICAL!)\n",
    "# ============================================================\n",
    "# This unifies labels from all datasets into a common format\n",
    "\n",
    "# Our unified 7 basic emotions\n",
    "UNIFIED_LABELS = ['happy', 'sad', 'angry', 'fear', 'surprise', 'disgust', 'neutral']\n",
    "\n",
    "# Translation map for each dataset\n",
    "LABEL_TRANSLATION = {\n",
    "    # simaanjali/emotion-analysis-based-on-text\n",
    "    'joy': 'happy',\n",
    "    'happiness': 'happy',\n",
    "    'love': 'happy',\n",
    "    'sadness': 'sad',\n",
    "    'grief': 'sad',\n",
    "    'anger': 'angry',\n",
    "    'rage': 'angry',\n",
    "    'annoyance': 'angry',\n",
    "    'fear': 'fear',\n",
    "    'anxiety': 'fear',\n",
    "    'nervousness': 'fear',\n",
    "    'surprise': 'surprise',\n",
    "    'shock': 'surprise',\n",
    "    'disgust': 'disgust',\n",
    "    'contempt': 'disgust',\n",
    "    'neutral': 'neutral',\n",
    "    'calm': 'neutral',\n",
    "    \n",
    "    # nelgiriyewithana/emotions\n",
    "    'happy': 'happy',\n",
    "    'sad': 'sad',\n",
    "    'angry': 'angry',\n",
    "    \n",
    "    # GoEmotions (28 -> 7 mapping)\n",
    "    'admiration': 'happy',\n",
    "    'amusement': 'happy',\n",
    "    'approval': 'happy',\n",
    "    'caring': 'happy',\n",
    "    'desire': 'happy',\n",
    "    'excitement': 'happy',\n",
    "    'gratitude': 'happy',\n",
    "    'optimism': 'happy',\n",
    "    'pride': 'happy',\n",
    "    'relief': 'happy',\n",
    "    'disappointment': 'sad',\n",
    "    'embarrassment': 'sad',\n",
    "    'remorse': 'sad',\n",
    "    'confusion': 'neutral',\n",
    "    'curiosity': 'neutral',\n",
    "    'realization': 'surprise',\n",
    "    'disapproval': 'angry',\n",
    "}\n",
    "\n",
    "def translate_label(label):\n",
    "    \"\"\"Translate any label to unified format.\"\"\"\n",
    "    label_lower = str(label).lower().strip()\n",
    "    return LABEL_TRANSLATION.get(label_lower, label_lower)\n",
    "\n",
    "# Convert to numeric labels\n",
    "LABEL_TO_ID = {label: i for i, label in enumerate(UNIFIED_LABELS)}\n",
    "ID_TO_LABEL = {i: label for i, label in enumerate(UNIFIED_LABELS)}\n",
    "\n",
    "print(f'Unified Labels: {UNIFIED_LABELS}')\n",
    "print(f'Label to ID: {LABEL_TO_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5: Load and Merge Datasets\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "all_texts = []\n",
    "all_labels = []\n",
    "\n",
    "# --- Dataset 1: simaanjali/emotion-analysis-based-on-text ---\n",
    "try:\n",
    "    for file in os.listdir('./datasets/text/simaanjali'):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(f'./datasets/text/simaanjali/{file}')\n",
    "            # Find text and label columns (may vary)\n",
    "            text_col = [c for c in df.columns if 'text' in c.lower() or 'content' in c.lower() or 'sentence' in c.lower()]\n",
    "            label_col = [c for c in df.columns if 'label' in c.lower() or 'emotion' in c.lower() or 'class' in c.lower()]\n",
    "            if text_col and label_col:\n",
    "                texts = df[text_col[0]].tolist()\n",
    "                labels = [translate_label(l) for l in df[label_col[0]].tolist()]\n",
    "                all_texts.extend(texts)\n",
    "                all_labels.extend(labels)\n",
    "                print(f'Loaded {len(texts)} samples from {file}')\n",
    "except Exception as e:\n",
    "    print(f'Error loading simaanjali dataset: {e}')\n",
    "\n",
    "# --- Dataset 2: nelgiriyewithana/emotions ---\n",
    "try:\n",
    "    for file in os.listdir('./datasets/text/emotions'):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(f'./datasets/text/emotions/{file}')\n",
    "            text_col = [c for c in df.columns if 'text' in c.lower()]\n",
    "            label_col = [c for c in df.columns if 'label' in c.lower() or 'emotion' in c.lower()]\n",
    "            if text_col and label_col:\n",
    "                texts = df[text_col[0]].tolist()\n",
    "                labels = [translate_label(l) for l in df[label_col[0]].tolist()]\n",
    "                all_texts.extend(texts)\n",
    "                all_labels.extend(labels)\n",
    "                print(f'Loaded {len(texts)} samples from {file}')\n",
    "except Exception as e:\n",
    "    print(f'Error loading emotions dataset: {e}')\n",
    "\n",
    "# --- Dataset 3: GoEmotions from HuggingFace ---\n",
    "try:\n",
    "    go_emotions = load_dataset('go_emotions', 'simplified', split='train')\n",
    "    GO_EMOTIONS_LABELS = ['admiration','amusement','anger','annoyance','approval','caring','confusion','curiosity','desire','disappointment','disapproval','disgust','embarrassment','excitement','fear','gratitude','grief','joy','love','nervousness','optimism','pride','realization','relief','remorse','sadness','surprise','neutral']\n",
    "    for item in go_emotions:\n",
    "        all_texts.append(item['text'])\n",
    "        # Use first label\n",
    "        label_idx = item['labels'][0] if item['labels'] else 27\n",
    "        original_label = GO_EMOTIONS_LABELS[label_idx]\n",
    "        all_labels.append(translate_label(original_label))\n",
    "    print(f'Loaded {len(go_emotions)} samples from GoEmotions')\n",
    "except Exception as e:\n",
    "    print(f'Error loading GoEmotions: {e}')\n",
    "\n",
    "print(f'\\nTotal samples: {len(all_texts)}')\n",
    "print(f'Label distribution: {pd.Series(all_labels).value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6: Convert to Numeric Labels and Create Dataset\n",
    "# ============================================================\n",
    "\n",
    "# Filter out invalid labels\n",
    "valid_data = [(t, l) for t, l in zip(all_texts, all_labels) if l in LABEL_TO_ID]\n",
    "texts_clean = [t for t, l in valid_data]\n",
    "labels_clean = [LABEL_TO_ID[l] for t, l in valid_data]\n",
    "\n",
    "print(f'Valid samples: {len(texts_clean)}')\n",
    "\n",
    "# Create HuggingFace Dataset\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_dict({'text': texts_clean, 'label': labels_clean})\n",
    "\n",
    "# Train/val/test split\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "train_test = dataset.train_test_split(test_size=0.2)\n",
    "test_val = train_test['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "dataset_dict = {\n",
    "    'train': train_test['train'],\n",
    "    'validation': test_val['train'],\n",
    "    'test': test_val['test']\n",
    "}\n",
    "\n",
    "print(f\"Train: {len(dataset_dict['train'])}\")\n",
    "print(f\"Val: {len(dataset_dict['validation'])}\")\n",
    "print(f\"Test: {len(dataset_dict['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 7: Tokenize and Prepare Model\n",
    "# ============================================================\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "MODEL_NAME = 'roberta-base'\n",
    "NUM_LABELS = len(UNIFIED_LABELS)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=ID_TO_LABEL,\n",
    "    label2id=LABEL_TO_ID\n",
    ")\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=128)\n",
    "\n",
    "tokenized_train = dataset_dict['train'].map(tokenize, batched=True)\n",
    "tokenized_val = dataset_dict['validation'].map(tokenize, batched=True)\n",
    "tokenized_test = dataset_dict['test'].map(tokenize, batched=True)\n",
    "\n",
    "print('Tokenization complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 8: Training\n",
    "# ============================================================\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(p.label_ids, preds),\n",
    "        'f1': f1_score(p.label_ids, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./text_model_unified',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print('Starting training...')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 9: Evaluate and Save\n",
    "# ============================================================\n",
    "results = trainer.evaluate(tokenized_test)\n",
    "print(f\"\\nTest Accuracy: {results['eval_accuracy']:.4f}\")\n",
    "print(f\"Test F1: {results['eval_f1']:.4f}\")\n",
    "\n",
    "# Save to local models folder\n",
    "SAVE_PATH = '../models/text_emotion_unified'\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "trainer.save_model(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "print(f'\\nModel saved to {SAVE_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 10: Upload to HuggingFace (Optional)\n",
    "# ============================================================\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Uncomment and run to upload\n",
    "login()  # Enter your HF token\n",
    "model.push_to_hub('BasantAwad/text-emotion-detction')\n",
    "tokenizer.push_to_hub('BasantAwad/text-emotion-detction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 11: Quick Test\n",
    "# ============================================================\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('text-classification', model=SAVE_PATH, top_k=3)\n",
    "\n",
    "test_texts = [\n",
    "    \"I'm so happy today!\",\n",
    "    \"This makes me really angry\",\n",
    "    \"I feel scared about tomorrow\",\n",
    "    \"ewww that's gross\",\n",
    "    \"wow I didn't expect that!\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = classifier(text)\n",
    "    print(f'\\n\"{text}\"')\n",
    "    for r in result[0]:\n",
    "        print(f\"  {r['label']}: {r['score']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
