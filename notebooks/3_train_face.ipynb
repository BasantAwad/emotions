{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Emotion Training - Multi-Dataset\n",
    "\n",
    "Trains Vision Transformer (ViT) on multiple face emotion datasets with unified labels.\n",
    "\n",
    "**Datasets:**\n",
    "- FER2013 (msambare/fer2013)\n",
    "- fahadullaha/facial-emotion-recognition-dataset\n",
    "- sujaykapadnis/emotion-recognition-dataset\n",
    "- ananthu017/emotion-detection-fer\n",
    "\n",
    "**Run in Google Colab with GPU!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1: Setup Kaggle API\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "kaggle_json = '''{\n",
    "    \"username\": \"basantawad\",\n",
    "    \"key\": \"73699caea5f0322acca5bc42516c5998\"\n",
    "}'''\n",
    "\n",
    "kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "kaggle_file = os.path.join(kaggle_dir, 'kaggle.json')\n",
    "with open(kaggle_file, 'w') as f:\n",
    "    f.write(kaggle_json)\n",
    "os.chmod(kaggle_file, 0o600)\n",
    "print('Kaggle API Connected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2: Install Libraries\n",
    "# ============================================================\n",
    "!pip install transformers datasets accelerate kaggle Pillow tqdm scikit-learn pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: Download Datasets from Kaggle\n",
    "# ============================================================\n",
    "!mkdir -p ./datasets/face\n",
    "\n",
    "# Dataset 1: FER2013\n",
    "!kaggle datasets download -d msambare/fer2013 -p ./datasets/face/fer2013 --unzip\n",
    "\n",
    "# Dataset 2: Facial Emotion Recognition\n",
    "!kaggle datasets download -d fahadullaha/facial-emotion-recognition-dataset -p ./datasets/face/fer_dataset --unzip\n",
    "\n",
    "# Dataset 3: Emotion Recognition Dataset\n",
    "!kaggle datasets download -d sujaykapadnis/emotion-recognition-dataset -p ./datasets/face/emotion_rec --unzip\n",
    "\n",
    "# Dataset 4: Emotion Detection FER\n",
    "!kaggle datasets download -d ananthu017/emotion-detection-fer -p ./datasets/face/emotion_det --unzip\n",
    "\n",
    "print('\\nDatasets downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4: Label Translation Map (CRITICAL!)\n",
    "# ============================================================\n",
    "UNIFIED_LABELS = ['happy', 'sad', 'angry', 'fear', 'surprise', 'disgust', 'neutral']\n",
    "\n",
    "LABEL_TRANSLATION = {\n",
    "    # FER2013 standard labels\n",
    "    'angry': 'angry',\n",
    "    'disgust': 'disgust',\n",
    "    'fear': 'fear',\n",
    "    'happy': 'happy',\n",
    "    'sad': 'sad',\n",
    "    'surprise': 'surprise',\n",
    "    'neutral': 'neutral',\n",
    "    \n",
    "    # Variations\n",
    "    'happiness': 'happy',\n",
    "    'sadness': 'sad',\n",
    "    'anger': 'angry',\n",
    "    'fearful': 'fear',\n",
    "    'surprised': 'surprise',\n",
    "    'disgusted': 'disgust',\n",
    "    \n",
    "    # Additional labels some datasets might have\n",
    "    'contempt': 'disgust',\n",
    "    'joy': 'happy',\n",
    "}\n",
    "\n",
    "def translate_label(label):\n",
    "    label_lower = str(label).lower().strip()\n",
    "    return LABEL_TRANSLATION.get(label_lower, label_lower)\n",
    "\n",
    "LABEL_TO_ID = {label: i for i, label in enumerate(UNIFIED_LABELS)}\n",
    "ID_TO_LABEL = {i: label for i, label in enumerate(UNIFIED_LABELS)}\n",
    "\n",
    "print(f'Unified Labels: {UNIFIED_LABELS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5: Load Images from All Datasets\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "image_paths = []\n",
    "image_labels = []\n",
    "\n",
    "def process_folder(base_path, dataset_name):\n",
    "    \"\"\"Process images organized in emotion folders.\"\"\"\n",
    "    count = 0\n",
    "    for emotion_folder in Path(base_path).iterdir():\n",
    "        if emotion_folder.is_dir():\n",
    "            folder_name = emotion_folder.name.lower()\n",
    "            translated = translate_label(folder_name)\n",
    "            if translated in UNIFIED_LABELS:\n",
    "                for img_file in emotion_folder.glob('*'):\n",
    "                    if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                        image_paths.append(str(img_file))\n",
    "                        image_labels.append(translated)\n",
    "                        count += 1\n",
    "    print(f'{dataset_name}: {count} images')\n",
    "\n",
    "# FER2013\n",
    "print('Loading FER2013...')\n",
    "if Path('./datasets/face/fer2013/train').exists():\n",
    "    process_folder('./datasets/face/fer2013/train', 'FER2013-train')\n",
    "if Path('./datasets/face/fer2013/test').exists():\n",
    "    process_folder('./datasets/face/fer2013/test', 'FER2013-test')\n",
    "\n",
    "# Other datasets - search recursively for emotion folders\n",
    "other_paths = [\n",
    "    './datasets/face/fer_dataset',\n",
    "    './datasets/face/emotion_rec',\n",
    "    './datasets/face/emotion_det'\n",
    "]\n",
    "\n",
    "for base in other_paths:\n",
    "    if Path(base).exists():\n",
    "        # Try direct processing\n",
    "        process_folder(base, base.split('/')[-1])\n",
    "        # Also try train/test subfolders\n",
    "        for sub in ['train', 'test', 'Train', 'Test', 'training', 'testing']:\n",
    "            sub_path = Path(base) / sub\n",
    "            if sub_path.exists():\n",
    "                process_folder(str(sub_path), f'{base.split(\"/\")[-1]}-{sub}')\n",
    "\n",
    "print(f'\\nTotal images: {len(image_paths)}')\n",
    "print(f'Label distribution: {dict(zip(*np.unique(image_labels, return_counts=True)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6: Create Dataset and Split\n",
    "# ============================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_ids = [LABEL_TO_ID[l] for l in image_labels]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    image_paths, label_ids, test_size=0.2, stratify=label_ids, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 7: Create HuggingFace Dataset\n",
    "# ============================================================\n",
    "from datasets import Dataset, Features, ClassLabel, Image as HFImage\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "# Create datasets\n",
    "train_data = Dataset.from_dict({'image': X_train, 'label': y_train}).cast_column('image', HFImage())\n",
    "val_data = Dataset.from_dict({'image': X_val, 'label': y_val}).cast_column('image', HFImage())\n",
    "test_data = Dataset.from_dict({'image': X_test, 'label': y_test}).cast_column('image', HFImage())\n",
    "\n",
    "# Load image processor and model (ViT for better accuracy)\n",
    "MODEL_NAME = 'google/vit-base-patch16-224'\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(UNIFIED_LABELS),\n",
    "    id2label=ID_TO_LABEL,\n",
    "    label2id=LABEL_TO_ID,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print('Model and processor loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 8: Preprocessing\n",
    "# ============================================================\n",
    "def preprocess(examples):\n",
    "    images = []\n",
    "    for img in examples['image']:\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        images.append(img)\n",
    "    inputs = processor(images=images, return_tensors='pt')\n",
    "    inputs['labels'] = examples['label']\n",
    "    return inputs\n",
    "\n",
    "train_data = train_data.map(preprocess, batched=True, batch_size=32, remove_columns=['image'])\n",
    "val_data = val_data.map(preprocess, batched=True, batch_size=32, remove_columns=['image'])\n",
    "test_data = test_data.map(preprocess, batched=True, batch_size=32, remove_columns=['image'])\n",
    "\n",
    "train_data.set_format('torch')\n",
    "val_data.set_format('torch')\n",
    "test_data.set_format('torch')\n",
    "\n",
    "print('Preprocessing complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 9: Training\n",
    "# ============================================================\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(p.label_ids, preds),\n",
    "        'f1': f1_score(p.label_ids, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./face_model_unified',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print('Starting training (this takes 1-2 hours)...')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 10: Evaluate and Save\n",
    "# ============================================================\n",
    "results = trainer.evaluate(test_data)\n",
    "print(f\"\\nTest Accuracy: {results['eval_accuracy']:.4f}\")\n",
    "print(f\"Test F1: {results['eval_f1']:.4f}\")\n",
    "\n",
    "# Save\n",
    "SAVE_PATH = '../models/face_emotion_unified'\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "trainer.save_model(SAVE_PATH)\n",
    "processor.save_pretrained(SAVE_PATH)\n",
    "print(f'\\nModel saved to {SAVE_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 11: Upload to HuggingFace (Optional)\n",
    "# ============================================================\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "model.push_to_hub('BasantAwad/facial-emotion')\n",
    "processor.push_to_hub('BasantAwad/facial-emotion')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
