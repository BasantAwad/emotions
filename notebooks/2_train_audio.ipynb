{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Emotion Training - Multi-Dataset\n",
    "\n",
    "Trains Wav2Vec 2.0 on multiple audio emotion datasets with unified labels.\n",
    "\n",
    "**Datasets:**\n",
    "- RAVDESS\n",
    "- dmitrybabko/speech-emotion-recognition-en\n",
    "- ejlok1/toronto-emotional-speech-set-tess\n",
    "\n",
    "**Run in Google Colab with GPU!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1: Setup Kaggle API\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "kaggle_json = '''{\n",
    "    \"username\": \"basantawad\",\n",
    "    \"key\": \"73699caea5f0322acca5bc42516c5998\"\n",
    "}'''\n",
    "\n",
    "kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "kaggle_file = os.path.join(kaggle_dir, 'kaggle.json')\n",
    "with open(kaggle_file, 'w') as f:\n",
    "    f.write(kaggle_json)\n",
    "os.chmod(kaggle_file, 0o600)\n",
    "print('Kaggle API Connected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2: Install Libraries\n",
    "# ============================================================\n",
    "!pip install transformers datasets accelerate kaggle librosa soundfile pandas scikit-learn numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: Download Datasets from Kaggle\n",
    "# ============================================================\n",
    "!mkdir -p ./datasets/audio\n",
    "\n",
    "# Dataset 1: RAVDESS\n",
    "!kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio -p ./datasets/audio/ravdess --unzip\n",
    "\n",
    "# Dataset 2: Speech Emotion Recognition EN\n",
    "!kaggle datasets download -d dmitrybabko/speech-emotion-recognition-en -p ./datasets/audio/ser_en --unzip\n",
    "\n",
    "# Dataset 3: TESS (Toronto Emotional Speech Set)\n",
    "!kaggle datasets download -d ejlok1/toronto-emotional-speech-set-tess -p ./datasets/audio/tess --unzip\n",
    "\n",
    "print('\\nDatasets downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4: Label Translation Map (CRITICAL!)\n",
    "# ============================================================\n",
    "UNIFIED_LABELS = ['happy', 'sad', 'angry', 'fear', 'surprise', 'disgust', 'neutral']\n",
    "\n",
    "# Translation map for audio datasets\n",
    "LABEL_TRANSLATION = {\n",
    "    # RAVDESS labels (from filename codes)\n",
    "    'neutral': 'neutral',\n",
    "    'calm': 'neutral',\n",
    "    'happy': 'happy',\n",
    "    'sad': 'sad',\n",
    "    'angry': 'angry',\n",
    "    'fearful': 'fear',\n",
    "    'fear': 'fear',\n",
    "    'disgust': 'disgust',\n",
    "    'surprised': 'surprise',\n",
    "    'surprise': 'surprise',\n",
    "    \n",
    "    # Speech Emotion Recognition EN\n",
    "    'happiness': 'happy',\n",
    "    'sadness': 'sad',\n",
    "    'anger': 'angry',\n",
    "    \n",
    "    # TESS labels\n",
    "    'ps': 'surprise',  # pleasant surprise\n",
    "    'pleasant_surprise': 'surprise',\n",
    "    'pleasantlytsurprised': 'surprise',\n",
    "}\n",
    "\n",
    "# RAVDESS emotion code mapping\n",
    "RAVDESS_CODES = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'neutral',  # calm -> neutral\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fear',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprise'\n",
    "}\n",
    "\n",
    "def translate_label(label):\n",
    "    label_lower = str(label).lower().strip()\n",
    "    return LABEL_TRANSLATION.get(label_lower, label_lower)\n",
    "\n",
    "LABEL_TO_ID = {label: i for i, label in enumerate(UNIFIED_LABELS)}\n",
    "ID_TO_LABEL = {i: label for i, label in enumerate(UNIFIED_LABELS)}\n",
    "\n",
    "print(f'Unified Labels: {UNIFIED_LABELS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5: Load and Process Audio Files\n",
    "# ============================================================\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "audio_files = []\n",
    "audio_labels = []\n",
    "\n",
    "# --- RAVDESS ---\n",
    "print('Loading RAVDESS...')\n",
    "ravdess_path = Path('./datasets/audio/ravdess')\n",
    "for wav_file in ravdess_path.rglob('*.wav'):\n",
    "    filename = wav_file.stem\n",
    "    parts = filename.split('-')\n",
    "    if len(parts) >= 3:\n",
    "        emotion_code = parts[2]\n",
    "        if emotion_code in RAVDESS_CODES:\n",
    "            audio_files.append(str(wav_file))\n",
    "            audio_labels.append(RAVDESS_CODES[emotion_code])\n",
    "print(f'RAVDESS: {len([l for l in audio_labels])} files')\n",
    "\n",
    "# --- TESS ---\n",
    "print('Loading TESS...')\n",
    "tess_path = Path('./datasets/audio/tess')\n",
    "tess_count = 0\n",
    "for wav_file in tess_path.rglob('*.wav'):\n",
    "    filename = wav_file.stem.lower()\n",
    "    # TESS format: OAF_angry_xxx or YAF_happy_xxx\n",
    "    parts = filename.split('_')\n",
    "    for part in parts:\n",
    "        translated = translate_label(part)\n",
    "        if translated in UNIFIED_LABELS:\n",
    "            audio_files.append(str(wav_file))\n",
    "            audio_labels.append(translated)\n",
    "            tess_count += 1\n",
    "            break\n",
    "print(f'TESS: {tess_count} files')\n",
    "\n",
    "# --- Speech Emotion Recognition EN ---\n",
    "print('Loading SER-EN...')\n",
    "ser_path = Path('./datasets/audio/ser_en')\n",
    "ser_count = 0\n",
    "for wav_file in ser_path.rglob('*.wav'):\n",
    "    # Try to extract emotion from folder name or filename\n",
    "    parent_name = wav_file.parent.name.lower()\n",
    "    translated = translate_label(parent_name)\n",
    "    if translated in UNIFIED_LABELS:\n",
    "        audio_files.append(str(wav_file))\n",
    "        audio_labels.append(translated)\n",
    "        ser_count += 1\n",
    "print(f'SER-EN: {ser_count} files')\n",
    "\n",
    "print(f'\\nTotal audio files: {len(audio_files)}')\n",
    "print(f'Label distribution: {dict(zip(*np.unique(audio_labels, return_counts=True)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6: Create Dataset and Split\n",
    "# ============================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert labels to IDs\n",
    "label_ids = [LABEL_TO_ID[l] for l in audio_labels]\n",
    "\n",
    "# Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    audio_files, label_ids, test_size=0.2, stratify=label_ids, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 7: Create PyTorch Dataset\n",
    "# ============================================================\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "class AudioEmotionDataset(Dataset):\n",
    "    def __init__(self, files, labels, feature_extractor, max_length=16000*5):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.fe = feature_extractor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio\n",
    "        wav, _ = librosa.load(self.files[idx], sr=SAMPLE_RATE)\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(wav) > self.max_length:\n",
    "            wav = wav[:self.max_length]\n",
    "        elif len(wav) < self.max_length:\n",
    "            wav = np.pad(wav, (0, self.max_length - len(wav)))\n",
    "        \n",
    "        # Process\n",
    "        inputs = self.fe(wav, sampling_rate=SAMPLE_RATE, return_tensors='pt', padding=False)\n",
    "        \n",
    "        return {\n",
    "            'input_values': inputs.input_values.squeeze(),\n",
    "            'labels': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "# Load feature extractor and model\n",
    "MODEL_NAME = 'facebook/wav2vec2-base'\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(UNIFIED_LABELS),\n",
    "    id2label=ID_TO_LABEL,\n",
    "    label2id=LABEL_TO_ID\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AudioEmotionDataset(X_train, y_train, feature_extractor)\n",
    "val_dataset = AudioEmotionDataset(X_val, y_val, feature_extractor)\n",
    "test_dataset = AudioEmotionDataset(X_test, y_test, feature_extractor)\n",
    "\n",
    "print('Datasets created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 8: Training\n",
    "# ============================================================\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(p.label_ids, preds),\n",
    "        'f1': f1_score(p.label_ids, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./audio_model_unified',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=500,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print('Starting training (this takes 1-2 hours)...')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 9: Evaluate and Save\n",
    "# ============================================================\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(f\"\\nTest Accuracy: {results['eval_accuracy']:.4f}\")\n",
    "print(f\"Test F1: {results['eval_f1']:.4f}\")\n",
    "\n",
    "# Save\n",
    "SAVE_PATH = '../models/audio_emotion_unified'\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "trainer.save_model(SAVE_PATH)\n",
    "feature_extractor.save_pretrained(SAVE_PATH)\n",
    "print(f'\\nModel saved to {SAVE_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 10: Upload to HuggingFace (Optional)\n",
    "# ============================================================\n",
    " from huggingface_hub import login\n",
    " login()\n",
    " model.push_to_hub('BasantAwad/speech_emotion')\n",
    " feature_extractor.push_to_hub('BasantAwad/speech_emotion')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
